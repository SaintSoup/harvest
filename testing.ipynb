{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn.preprocessing \n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.signal import butter, lfilter\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "source": [
    "# This part is for data preprocessing\n",
    "### input: \n",
    "The data stream (sensor data)\n",
    "### output: \n",
    "The np.array with the format of deltaT, the previous data, and the label is the next max"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-844b53faedb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#physical_devices('GPU')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Num GPUs Available: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU') #physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "source": [
    "# Convention for datasets\n",
    "The base time step is in milliseconds\n",
    "\n",
    "The order of acceleration axes is by default (x,y,z) and the data is by default selected according to the x axis. To make modifications to this configuration, command-line flags are available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-5e10f8f73bab>, line 11)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5e10f8f73bab>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    for i in range(1,len(data)):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def import_data(path_to_dataset,seperator=',',sampling_freq=1000,order=['ax','ay','az','time'],abstime=0,stamp=0): \n",
    "    #imports data, seperated by a comma (default), with a 1 kHz frequency\n",
    "    #the order of the data columns by default ax, ay, az, and are the timestamps provided              (abstime=1) or are timestamps in delta times and a list of data to keep\n",
    "    data_init = pd.read_csv(filepath_or_buffer=path_to_dataset,sep=seperator, header=0,names=order)\n",
    "    data_init.header('dt')\n",
    "    if data_init.header('dt') != None:\n",
    "        #Adding A cummulative sum comlun(for absolute time), this will serve as identification and         will help during plotting\n",
    "        data_init[\"time\"]=data_init[\"dt\"].cumsum()\n",
    "    #----------------------------------------\n",
    "    # \n",
    "    # if stamp:\n",
    "    #To generate a time stamp \n",
    "    #    data_init[\"timestamp\"]= data_init[\"time\"].apply(lambda x: datetime(microseconds = x*))\n",
    "    #\n",
    "    #----------------------------------------\n",
    "\n",
    "    #data_init[][len(data_init)-1]\n",
    "    data_init.drop(headers=[\"az\",\"ay\"])\n",
    "\n",
    "    data_out=pd.headers(data_init.headers())\n",
    "    #Homogenizing the sampling rate (we settle for the case where delta t is 1)\n",
    "    num_cols = len(data_init.columns())\n",
    "    j=0\n",
    "    for index in len(data_init):\n",
    "        if data_init[\"dt\"].loc(index) == 1:\n",
    "            data_out.loc[j] =data_init[index]\n",
    "            j+=1\n",
    "        else:\n",
    "            for i in range(0,data_init[\"dt\"].loc()):\n",
    "                data_out.loc[j] = [np.NaN for p in range(0,num_cols)]\n",
    "                j+=1\n",
    "            data_out.loc[j] =data_init[index]\n",
    "            j+=1\n",
    "    data_init.drop(headers=[\"dt\"])\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe(df):\n",
    "    print(\"-------Information about the dataframe-------\")\n",
    "    df.info()\n",
    "    df.describe()\n",
    "    print(\"-------Head and Tail of the dataframe-------\")\n",
    "    df.Head()\n",
    "    df.Tail()\n",
    "    if df.value_counts():\n",
    "        print(\"-------Count of missing data in the dataframe-------\")\n",
    "        df.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scope(xaxis,xlabel=\"\",yaxis,ylabel=\"\",step=1, join=0):\n",
    "    #takes in an xaxis and a yaxis(could contain headers)\n",
    "    if len(yaxis.columns())-1:\n",
    "        +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(method='linear',df,fs=1000,cutoff,order=2):\n",
    "    #takes in a dataframe, a sampling frequency (1 kHz by default), a cutoff frquency, and an order and applies a lowpass butterworth filter to the acceleration data\n",
    "    df[\"ax\"]=df[\"ax\"].interpolate(method=method, limit_direction='forward')\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    data=df[\"ax\"].to_numpy()\n",
    "    y = lfilter(b, a, data)\n",
    "    data_out =pd.dataframe(y)\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extremes(df, fs):\n",
    "    #determining the extrema \n",
    "    df['min'] = df['ax'][(df['ax'].shift(1) > df['ax']) & (df['ax'].shift(-1) > df['ax'])]\n",
    "    df['max'] = df['ax'][(df['ax'].shift(1) < df['ax']) & (df['ax'].shift(-1) < df['ax'])]\n",
    "    #computing the difference between consecutive extrema as well as the time difference between each extreme  (following)\n",
    "    min=df['ax'].loc[0]\n",
    "    max=df['ax'].loc[0]\n",
    "    delta_t=0\n",
    "    i_last_min=0\n",
    "    i_last_max=0\n",
    "    for index in len(df)\n",
    "        if df['min'].loc[index]:\n",
    "            min= df['min'].loc[index]\n",
    "            df['delta_ext'].loc[index]=max-min\n",
    "            df['next_ext'].loc[i_last_min] = delta_t\n",
    "            delta_t=0\n",
    "            i_last_min=index\n",
    "        if df['max'].loc[index]:\n",
    "            max= df['max'].loc[index]\n",
    "            df['delta_ext'].loc[index]=max-min \n",
    "            df['next_ext'].loc[i_last_max] = delta_t\n",
    "            delta_t=0\n",
    "            i_last_min=index\n",
    "        delta_t+= 1/fs\n",
    "        #and then you fill the empty slots with a linear interpolation of the time \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(df):\n",
    "    #Introduce the colum\n",
    "    df=df.interpolate(method='spline',order=5, limit_direction='forward')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_(df,headers,outputs, win, train_ratio):\n",
    "#Manipulates the entry data into tensors of window size and then the model output should be min max and delta_next to provide history for each: df (datafame), HEADER (string of the column to reshape), win (the past values) \n",
    "#make a tensor (window(),2,len(sequence))\n",
    "    for i in range(0, win):\n",
    "        #shift it win times\n",
    "        df[str(HEADER,i)] = df[HEADER].shift(-i)\n",
    "    \n",
    "        \n",
    "    return df\n",
    "#unifinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_model(gate,win,activation='relu'):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(win,activation,input_shape=(win,2,)))\n",
    "    model.add(keras.layers.GRU(gate))\n",
    "    model.add(keras.layers.Dense(2))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(gate,win,activation='relu'):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(win,activation,input_shape=(win,2,)))\n",
    "    model.add(keras.layers.lstm(gate))\n",
    "    model.add(keras.layers.Dense(2))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rnn_model(gate,win,activation='relu'):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(win,activation,input_shape=(win,2,)))\n",
    "    model.add(keras.layers.SimpleRNN(gate))\n",
    "    model.add(keras.layers.Dense(2))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_model(gate,win,activation='relu'): # experimental\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(win,activation,input_shape=(win,2,)))\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.SimpleRNN(gate)))\n",
    "    model.add(keras.layers.Dense(2))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_train_test(model,X_train, X_val, Y_train, Y_val, X_test, Y_test, epochs, batches, loss='mse', opt='rmsprop', metrics):\n",
    "    model.compile(optimizer=opt, loss = loss, metrics=metrics)\n",
    "    model.summary()\n",
    "    hist= model.fit(X_train,Y_train, epochs=epochs, batch_size= batches, validation_data=(X_val, Y_val))\n",
    "    pred = model.evaluate(X_test,Y_test)\n",
    "    loss = hist.history['loss']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    epochs = range(1, len(loss) +1 )\n",
    "    plt.clf()\n",
    "    plt.plot(epochs, loss, 'g.', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b.', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #plt.clf()\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('harvest': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "b332d3d07aa9fece0702e4ef1b3d3580dbb9ef667445f63ca0a0c229ca954555"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}